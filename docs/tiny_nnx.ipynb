{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny NNX\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cgarciae/nnx/blob/main/docs/tiny_nnx.ipynb)\n",
    "\n",
    "A pedagogical implementation of NNX's core APIs.\n",
    "\n",
    "## Core APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import typing as tp\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import dataclasses\n",
    "\n",
    "A = tp.TypeVar(\"A\") \n",
    "M = tp.TypeVar(\"M\", bound=\"Module\")\n",
    "Sharding = tp.Tuple[tp.Optional[str], ...]\n",
    "KeyArray = random.KeyArray\n",
    "\n",
    "\n",
    "class Variable(tp.Generic[A]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection: str,\n",
    "        value: A,\n",
    "        *,\n",
    "        sharding: tp.Optional[Sharding] = None,\n",
    "    ):\n",
    "        self.value = value\n",
    "        self.collection = collection\n",
    "        self.sharding = sharding\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Variable(value={self.value}, collection={self.collection}, sharding={self.sharding})\"\n",
    "    \n",
    "\n",
    "jax.tree_util.register_pytree_node(\n",
    "    Variable,\n",
    "    lambda x: ((x.value,), (x.collection, x.sharding)),\n",
    "    lambda metadata, value: Variable(metadata[0], value[0], sharding=metadata[1]),\n",
    ")\n",
    "\n",
    "class State(dict[str, Variable[tp.Any]]):\n",
    "    def filter(self, collection: str) -> \"State\":\n",
    "        return State(\n",
    "            {\n",
    "                path: variable\n",
    "                for path, variable in self.items()\n",
    "                if variable.collection == collection\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        elems = \",\\n  \".join(f\"'{path}': {variable}\".replace(\"\\n\", \"\\n    \") for path, variable in self.items())\n",
    "        return f\"State({{\\n  {elems}\\n}})\"\n",
    "\n",
    "\n",
    "jax.tree_util.register_pytree_node(\n",
    "    State,\n",
    "    # in reality, values and paths should be sorted by path\n",
    "    lambda x: (tuple(x.values()), tuple(x.keys())),\n",
    "    lambda paths, values: State(dict(zip(paths, values))),\n",
    ")\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class ModuleDef(tp.Generic[M]):\n",
    "    type: tp.Type[M]\n",
    "    index: int\n",
    "    submodules: tp.Dict[str, tp.Union[\"ModuleDef[Module]\", int]]\n",
    "    static_fields: tp.Dict[str, tp.Any]\n",
    "\n",
    "    def apply(self, state: State) -> tp.Callable[..., tuple[tp.Any, tuple[State, \"ModuleDef[M]\"]]]:\n",
    "        def _apply(*args, **kwargs):\n",
    "            module = self.merge(state)\n",
    "            out = module(*args, **kwargs) # type: ignore\n",
    "            return out, module.partition()\n",
    "        return _apply\n",
    "\n",
    "    def merge(self, state: State) -> M:\n",
    "        module = ModuleDef._build_module_recursive(self, {})\n",
    "        module.update_state(state)\n",
    "        return module\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_module_recursive(\n",
    "        moduledef: tp.Union[\"ModuleDef[M]\", int],\n",
    "        index_to_module: tp.Dict[int, \"Module\"],\n",
    "    ) -> M:\n",
    "        if isinstance(moduledef, int):\n",
    "            return index_to_module[moduledef] # type: ignore\n",
    "\n",
    "        assert moduledef.index not in index_to_module\n",
    "\n",
    "        # add a dummy module to the index to avoid infinite recursion\n",
    "        module = object.__new__(moduledef.type)\n",
    "        index_to_module[moduledef.index] = module\n",
    "\n",
    "        submodules = {\n",
    "            name: ModuleDef._build_module_recursive(submodule, index_to_module)\n",
    "            for name, submodule in moduledef.submodules.items()\n",
    "        }\n",
    "        vars(module).update(moduledef.static_fields)\n",
    "        vars(module).update(submodules)\n",
    "        return module\n",
    "\n",
    "\n",
    "class Module:\n",
    "    def partition(self: M) -> tp.Tuple[State, ModuleDef[M]]:\n",
    "        state = State()\n",
    "        moduledef = Module._partition_recursive(\n",
    "            module=self, module_id_to_index={}, path_parts=(), state=state)\n",
    "        assert isinstance(moduledef, ModuleDef)\n",
    "        return state, moduledef\n",
    "\n",
    "    @staticmethod\n",
    "    def _partition_recursive(\n",
    "        module: M,\n",
    "        module_id_to_index: tp.Dict[int, int],\n",
    "        path_parts: tp.Tuple[str, ...],\n",
    "        state: State,\n",
    "    ) -> tp.Union[ModuleDef[M], int]:\n",
    "        if id(module) in module_id_to_index:\n",
    "            return module_id_to_index[id(module)]\n",
    "\n",
    "        index = len(module_id_to_index)\n",
    "        module_id_to_index[id(module)] = index\n",
    "\n",
    "        submodules = {}\n",
    "        static_fields = {}\n",
    "\n",
    "        # iterate fields sorted by name to ensure deterministic order\n",
    "        for name, value in sorted(vars(module).items(), key=lambda x: x[0]):\n",
    "            value_path = (*path_parts, name)\n",
    "            # if value is a Module, recurse\n",
    "            if isinstance(value, Module):\n",
    "                submoduledef = Module._partition_recursive(\n",
    "                    value, module_id_to_index, value_path, state)\n",
    "                submodules[name] = submoduledef\n",
    "            # if value is a Variable, add to state\n",
    "            elif isinstance(value, Variable):\n",
    "                state[\"/\".join(value_path)] = value\n",
    "            else: # otherwise, add to static fields\n",
    "                static_fields[name] = value\n",
    "\n",
    "        return ModuleDef(\n",
    "            type=type(module),\n",
    "            index=index,\n",
    "            submodules=submodules,\n",
    "            static_fields=static_fields,\n",
    "        )\n",
    "\n",
    "    def update_state(self, state: State) -> None:\n",
    "        for path, value in state.items():\n",
    "            path_parts = path.split(\"/\")\n",
    "            Module._set_value_at_path(self, path_parts, value)\n",
    "\n",
    "    @staticmethod\n",
    "    def _set_value_at_path(module: \"Module\", path_parts: tp.Sequence[str], value: Variable[tp.Any]) -> None:\n",
    "        if len(path_parts) == 1:\n",
    "            setattr(module, path_parts[0], value)\n",
    "        else:\n",
    "            Module._set_value_at_path(getattr(module, path_parts[0]), path_parts[1:], value)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Context:\n",
    "    key: KeyArray\n",
    "    count: int = 0\n",
    "    count_path: tuple[int, ...] = ()\n",
    "\n",
    "    def fork(self) -> \"Context\":\n",
    "        \"\"\"Forks the context, guaranteeing that all the random numbers generated\n",
    "        will be different from the ones generated in the original context. Fork is\n",
    "        used to create a new Context that can be passed to a JAX transform\"\"\"\n",
    "        count_path = self.count_path + (self.count,)\n",
    "        self.count += 1\n",
    "        return Context(self.key, count_path=count_path)\n",
    "\n",
    "    def make_rng(self) -> jax.Array:\n",
    "        fold_data = self._stable_hash(self.count_path + (self.count,))\n",
    "        self.count += 1\n",
    "        return random.fold_in(self.key, fold_data) # type: ignore\n",
    "\n",
    "    @staticmethod\n",
    "    def _stable_hash(data: tuple[int, ...]) -> int:\n",
    "        hash_str = \" \".join(str(x) for x in data)\n",
    "        _hash = hashlib.blake2s(hash_str.encode())\n",
    "        hash_bytes = _hash.digest()\n",
    "        # uint32 is represented as 4 bytes in big endian\n",
    "        return int.from_bytes(hash_bytes[:4], byteorder=\"big\")\n",
    "\n",
    "# in the real NNX Context is not a pytree, instead\n",
    "# it has a partition/merge API similar to Module\n",
    "# but for simplicity we use a pytree here\n",
    "jax.tree_util.register_pytree_node(\n",
    "    Context,\n",
    "    lambda x: ((x.key,),(x.count, x.count_path)),\n",
    "    lambda metadata, value: Context(value[0], *metadata),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, din: int, dout: int, *, ctx: Context):\n",
    "        self.din = din\n",
    "        self.dout = dout\n",
    "        key = ctx.make_rng()\n",
    "        self.w = Variable(\"params\", random.uniform(key, (din, dout)))\n",
    "        self.b = Variable(\"params\", jnp.zeros((dout,)))\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return x @ self.w.value + self.b.value\n",
    "\n",
    "class BatchNorm(Module):\n",
    "    def __init__(self, din: int, mu: float = 0.95):\n",
    "        self.mu = mu\n",
    "        self.scale = Variable(\"params\", jax.numpy.ones((din,)))\n",
    "        self.bias = Variable(\"params\", jax.numpy.zeros((din,)))\n",
    "        self.mean = Variable(\"batch_stats\", jax.numpy.zeros((din,)))\n",
    "        self.var = Variable(\"batch_stats\", jax.numpy.ones((din,)))\n",
    "\n",
    "    def __call__(self, x, train: bool) -> jax.Array:\n",
    "        if train:\n",
    "            axis = tuple(range(x.ndim - 1))\n",
    "            mean = jax.numpy.mean(x, axis=axis, keepdims=True)\n",
    "            var = jax.numpy.var(x, axis=axis, keepdims=True)\n",
    "            # ema update\n",
    "            self.mean.value = self.mu * self.mean.value + (1 - self.mu) * mean\n",
    "            self.var.value = self.mu * self.var.value + (1 - self.mu) * var\n",
    "        else:\n",
    "            mean, var = self.mean.value, self.var.value\n",
    "\n",
    "        scale, bias = self.scale.value, self.bias.value\n",
    "        x = (x - mean) / jax.numpy.sqrt(var + 1e-5) * scale + bias\n",
    "        return x\n",
    "    \n",
    "class Dropout(Module):\n",
    "    def __init__(self, rate: float):\n",
    "        self.rate = rate\n",
    "\n",
    "    def __call__(self, x: jax.Array, *, train: bool, ctx: Context) -> jax.Array:\n",
    "        if train:\n",
    "            mask = random.bernoulli(ctx.make_rng(), (1 - self.rate), x.shape)\n",
    "            x = x * mask / (1 - self.rate)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scan Over Layers Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Block(Module):\n",
    "    def __init__(self, din: int, dout: int, *, ctx: Context):\n",
    "        self.linear = Linear(din, dout, ctx=ctx)\n",
    "        self.bn = BatchNorm(dout)\n",
    "        self.dropout = Dropout(0.1)\n",
    "\n",
    "    def __call__(self, x: jax.Array, *, train: bool, ctx: Context) -> jax.Array:\n",
    "        x = self.linear(x)\n",
    "        x = self.bn(x, train=train)\n",
    "        x = jax.nn.gelu(x)\n",
    "        x = self.dropout(x, train=train, ctx=ctx)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ScanMLP(Module):\n",
    "    def __init__(self, hidden_size: int, n_layers: int, *, ctx: Context):\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # lift init\n",
    "        key = random.split(ctx.make_rng(), n_layers - 1)\n",
    "        moduledef: ModuleDef[Block] = None # type: ignore\n",
    "\n",
    "        def init_fn(key):\n",
    "            nonlocal moduledef\n",
    "            state, moduledef = Block(hidden_size, hidden_size, ctx=Context(key)).partition()\n",
    "            return state\n",
    "        \n",
    "        state = jax.vmap(init_fn)(key)\n",
    "        self.layers = moduledef.merge(state)\n",
    "        self.linear = Linear(hidden_size, hidden_size, ctx=ctx)\n",
    "\n",
    "    def __call__(self, x: jax.Array, *, train: bool, ctx: Context) -> jax.Array:\n",
    "        # lift call\n",
    "        key: jax.Array = random.split(ctx.make_rng(), self.n_layers - 1) # type: ignore\n",
    "        state, moduledef = self.layers.partition()\n",
    "\n",
    "        def scan_fn(x, inputs: tuple[jax.Array, State]):\n",
    "            key, state = inputs\n",
    "            x, (state, _) = moduledef.apply(state)(\n",
    "                x, train=train, ctx=Context(key)\n",
    "            )\n",
    "            return x, state\n",
    "        \n",
    "        x, state = jax.lax.scan(scan_fn, x, (key, state))\n",
    "        self.layers.update_state(state)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = State({\n",
      "  'layers/bn/bias': Variable(value=(4, 10), collection=params, sharding=None),\n",
      "  'layers/bn/mean': Variable(value=(4, 1, 10), collection=batch_stats, sharding=None),\n",
      "  'layers/bn/scale': Variable(value=(4, 10), collection=params, sharding=None),\n",
      "  'layers/bn/var': Variable(value=(4, 1, 10), collection=batch_stats, sharding=None),\n",
      "  'layers/linear/b': Variable(value=(4, 10), collection=params, sharding=None),\n",
      "  'layers/linear/w': Variable(value=(4, 10, 10), collection=params, sharding=None),\n",
      "  'linear/b': Variable(value=(10,), collection=params, sharding=None),\n",
      "  'linear/w': Variable(value=(10, 10), collection=params, sharding=None)\n",
      "})\n",
      "moduledef = ModuleDef(type=<class '__main__.ScanMLP'>, index=0, submodules={'layers': ModuleDef(type=<class '__main__.Block'>, index=1, submodules={'bn': ModuleDef(type=<class '__main__.BatchNorm'>, index=2, submodules={}, static_fields={'mu': 0.95}), 'dropout': ModuleDef(type=<class '__main__.Dropout'>, index=3, submodules={}, static_fields={'rate': 0.1}), 'linear': ModuleDef(type=<class '__main__.Linear'>, index=4, submodules={}, static_fields={'din': 10, 'dout': 10})}, static_fields={}), 'linear': ModuleDef(type=<class '__main__.Linear'>, index=5, submodules={}, static_fields={'din': 10, 'dout': 10})}, static_fields={'n_layers': 5})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "module = ScanMLP(hidden_size=10, n_layers=5, ctx=Context(random.PRNGKey(0)))\n",
    "x = jax.random.normal(random.PRNGKey(0), (2, 10))\n",
    "y = module(x, train=True, ctx=Context(random.PRNGKey(1)))\n",
    "\n",
    "state, moduledef = module.partition()\n",
    "print(\"state =\", jax.tree_map(jnp.shape, state))\n",
    "print(\"moduledef =\", moduledef)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = State({\n",
      "  'layers/bn/bias': Variable(value=(4, 10), collection=params, sharding=None),\n",
      "  'layers/bn/scale': Variable(value=(4, 10), collection=params, sharding=None),\n",
      "  'layers/linear/b': Variable(value=(4, 10), collection=params, sharding=None),\n",
      "  'layers/linear/w': Variable(value=(4, 10, 10), collection=params, sharding=None),\n",
      "  'linear/b': Variable(value=(10,), collection=params, sharding=None),\n",
      "  'linear/w': Variable(value=(10, 10), collection=params, sharding=None)\n",
      "})\n",
      "batch_stats = State({\n",
      "  'layers/bn/mean': Variable(value=(4, 1, 10), collection=batch_stats, sharding=None),\n",
      "  'layers/bn/var': Variable(value=(4, 1, 10), collection=batch_stats, sharding=None)\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# split\n",
    "params = state.filter(\"params\")\n",
    "batch_stats = state.filter(\"batch_stats\")\n",
    "# merge\n",
    "state = State({**params, **batch_stats})\n",
    "\n",
    "print(\"params =\", jax.tree_map(jnp.shape, params))\n",
    "print(\"batch_stats =\", jax.tree_map(jnp.shape, batch_stats))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
